#!/usr/bin/env python3


from transformers import AutoTokenizer
from ray.serve.experimental.llm.models.opt import OPT
from ray.serve.experimental.llm.models.casual_lm import CausalLM
from ray.serve.experimental.llm.worker import InferenceWorker
from ray.serve.experimental.llm.types import SamplingParams
from ray.serve.experimental.llm.policy import QuotaBasedRequestSelectionPolicy, StaticBatchPolicy, RequestSelectionPolicy
from ray.serve.experimental.llm.queue import InferenceRequest, RequestQueue
from ray.serve.experimental.llm.scheduler import InferenceScheduler, TransfomerTokenizer
from copy import deepcopy
import pytest
import torch
import uvicorn
from fastapi.responses import StreamingResponse
from fastapi import FastAPI, Request
import deepspeed
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline
import argparse
import asyncio
import time
from typing import List, Dict, Optional
import json
import os
os.environ["TRANSFORMERS_CACHE"] = '/data/cache'


MAX_LEN = 512
GEN_LEN = 128

app = FastAPI()


class ImpatientStaticBatchPolicy(RequestSelectionPolicy):
    def __init__(
        self,
        batch_size: int,
    ):
        self.batch_size = batch_size

    def select_new_requests(
        self, in_process_requests: List[InferenceRequest], queue: RequestQueue, has_oom: bool
    ) -> List[InferenceRequest]:
        if in_process_requests:
            return []

        results = []
        while not queue.empty() and len(results) < self.batch_size:
            request = queue.peek()
            results.append(request)
            queue.pop()

        return results


class FastAPIServer:
    def __init__(self, policy, model_name, max_batch_total_tokens, static_batch_size):
        self.policy = policy
        self.model_name = model_name
        self.model_ready = asyncio.Event()
        self.init_model(max_batch_total_tokens, static_batch_size)

        self.waiting_requests = []
        self.responses = []
        self.start_event = asyncio.Event()
        self.end_event = asyncio.Event()
        self.waiting_expectation = None

        self.test_tokenizer = AutoTokenizer.from_pretrained(model_name)

    def init_model(self, max_batch_total_tokens, static_batch_size):
        print(f'Init model {max_batch_total_tokens=}')
        self._inference_mode_raii_guard = torch._C._InferenceMode(True)
        assert 'opt' in self.model_name.lower()

        if self.policy == "static":
            request_selection_policy = StaticBatchPolicy(static_batch_size)
        elif self.policy == "impatient_static":
            request_selection_policy = ImpatientStaticBatchPolicy(
                static_batch_size)
        elif self.policy == "quota":
            request_selection_policy = QuotaBasedRequestSelectionPolicy(
                max_batch_total_tokens=max_batch_total_tokens, max_waiting_tokens=7
            )
        else:
            raise ValueError(f"unknown policy {self.policy=}")

        def create_model():
            model = OPT(self.model_name)
            self.model_ready.set()
            return model

        self.scheduler = InferenceScheduler(
            tokenizer=TransfomerTokenizer(
                pretrained_model_name_or_path=self.model_name, padding_side="left"
            ),
            inference_worker_loader=lambda: InferenceWorker(create_model),
            request_selection_policy=request_selection_policy,
            request_queue=RequestQueue(),
            loop=None,
            inline=False,
        )

    async def generate(self, request_dict: Dict):

        prompt = request_dict['inputs']
        parameters = request_dict['parameters']

        assert 'reponse_len' in parameters
        assert 'prompt_len' in parameters

        max_new_tokens = parameters.get('reponse_len', GEN_LEN)
        sampling_params = SamplingParams(
            temperature=1.0,
            repetition_penalty=1.0,
            top_k=0,
            top_p=1.0,
            typical_p=1.0,
            do_sample=False,
            max_new_tokens=max_new_tokens,
            stop_sequences=[],
            ignore_eos_token=True,
            watermark=False,
            seed=42,
        )

        max_length = parameters.get('prompt_len', MAX_LEN)

        result = await self.scheduler.async_process_request(
            prompt, sampling_params, max_length=max_length)

        # calculate generated tokens.
        prompt_ids = self.test_tokenizer(prompt)['input_ids']
        output_ids = self.test_tokenizer(result)['input_ids']
        actual_ray_gen_len = len(output_ids)
        # print(f'prompt_ids {max_length=} {len(prompt_ids)=} {prompt_ids=}')
        # print(f'output_ids {max_new_tokens=} {len(output_ids)=} {output_ids=}')

        print(f'desired len {max_new_tokens} actual len {actual_ray_gen_len}')

        result = {
            'ray_gen_len': max_new_tokens,
            'generated_text': result,
        }
        return result

    async def is_ready(self):
        return self.model_ready.is_set()


@app.post("/generate")
async def generate_stream(request: Request):
    request_dict = await request.json()
    return await server.generate(request_dict)


@app.get("/is_ready")
async def is_ready(request: Request):
    return await server.is_ready()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, required=True)
    parser.add_argument(
        '--policy', choices=["impatient_static", "static", "quota"], default="quota")
    parser.add_argument('--model-name', type=str, required=True)

    parser.add_argument('--max-batch-total-tokens', type=int)
    parser.add_argument('--static-batch-size', type=int)
    args = parser.parse_args()

    if args.policy == 'quota':
        assert args.max_batch_total_tokens is not None
        assert args.static_batch_size is None

    if args.policy == 'static':
        assert args.max_batch_total_tokens is None
        assert args.static_batch_size is not None

    if args.policy == 'impatient_static':
        assert args.max_batch_total_tokens is None
        assert args.static_batch_size is not None

    server = FastAPIServer(args.policy, args.model_name,
                           args.max_batch_total_tokens, args.static_batch_size)

    uvicorn.run(app, host='localhost', port=args.port, log_level="info")
